---
title: "Chapter4"
output: html_document
date: "2023-11-22"
---

# Clustering and classification

This week we are practicing with a Boston data from the MASS package. First lets load the data and observe the structure and dimensions of it:

```{r  fig.width=5, fig.height=4, message=FALSE, warning=FALSE}

# accessing the data

library(MASS)
library(corrplot)
library(tidyr)
data("boston")

# structure and dimensions
str(Boston)
dim(Boston)


```

The data describes housing values in suburbs of boston and it has 506 observations and 14 variables. The meanings of each individual variable can be found through this link: https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html.

Next lets observe the graphical overview of the data:

```{r}

summary(Boston)

cor_matrix <- cor(Boston) %>% round(digits = 2)

corrplot(cor_matrix, method="shade", title = "correlations between variables in the data")

```

There is a high correlation between accessibility to radial highways (variable "rad") and full-value property tax (variable "tax"). Which means that the homes closer to highways are more expensive. There is also an expected negative correlation between median value of owner occupied homes (variable "medv") and lower status of the population (variable "lstat"). Which means that in the areas where the population have lower economic status the homes are also less valuable. There is also a strong negative correlation between the proportion on owner occupied homes built prior to 1940 (variable "age") and the weighted mean distance to five Boston employment centers (variable "dis"), which means that the older owner-occupied houses are further away from the city center and Bostons services. 

For further investigations of the data, lets scale the dataset:

```{r  fig.width=5, fig.height=5, message=FALSE, warning=FALSE}

boston_scaled <- scale(Boston)

# summaries of the scaled variables

summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

boston_scaled <- as.data.frame(boston_scaled)

```

All the variables are now divided by their means, so the mean of all the variables is 0.

Let's make a new variable of the crime rate of Boston:

```{r message=FALSE, warning=FALSE}

# new categorical crime varibale
bins <- quantile(boston_scaled$crim)
bins 
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low","med_low","med_high","high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```


Now lets divide the dataset to train and test sets:

```{r message=FALSE, warning=FALSE}
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train and test set
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]


```


Then lets execute an linear discriminant analysis of the train set as the crime rate as the target variable:

```{r message=FALSE, warning=FALSE}

lda.fit <- lda(crime ~ ., data = train)
lda.fit

# plotting the analysis

lda.arrows <- function(x, myscale = 1, arrow_heads = 0.2, color = "black", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)}

classes <- as.numeric(train$crime)

plot(lda.fit, dimen = 2, col = classes)
lda.arrows(lda.fit, myscale = 1)

```

IT seems that the variable rad, i.e, index of accessibility to radial highways, have significance in this model with some classes.


Let's observe this more closely by predicting the classes with the LDA and cross tabulate the results:


```{r message=FALSE, warning=FALSE}
# saving the correct classes from test data
correct_classes <- test$crime

# removing the crime variable from test data
test <- dplyr::select(test, -crime)

# predictions
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)


```

The predictions for high is quite good, only 3 observations were predicted wrongly to the med_high class. But the med_low predictions were not so good. 

Lets take the the Boston dataset again and calculate the usual distances between vectors (fist_euc) and the absolute distances between vectors (dist_man):

```{r message=FALSE, warning=FALSE}
# reloading the dataset
data("Boston")

# standardizing the dataset
scaled_boston <- scale(Boston)
scaled_data <- as.data.frame(scaled_boston)

dist_eu <- dist(scaled_data ,method = "euclidean")
summary(dist_eu)

dist_man <- dist(scaled_data, method = "manhattan")
summary(dist_man)
```

Let's then check the k-means of the Boston dataset:

```{r message=FALSE, warning=FALSE}

# running the k-means analysis
km <- kmeans(scaled_data, centers = 4)

## The pairs are reduced so the data is observable
pairs(scaled_data[c("age", "medv", "lstat", "nox", "zn")], col = km$cluster)

```

Lets then investigate what the optimal number of clusters would be:

```{r message=FALSE, warning=FALSE}

library(ggplot2)
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(scaled_data[c("age", "medv", "lstat", "nox", "zn")], k)$tot.withinss})

# visualize the results
qplot(x = 1:10, y = twcss, geom = 'line')

```
From the plot we can see that the when the number of clusters go over 10 the results are not that much better. So two clusters give the best results.

Let's therefore change the k-means clusters to two:

```{r message=FALSE, warning=FALSE}
# k-means clustering
km <- kmeans(scaled_data[c("age", "medv", "lstat", "nox", "zn")], centers = 2)

# plot the Boston dataset with clusters
pairs(scaled_data[c("age", "medv", "lstat", "nox", "zn")], col = km$cluster)
```

It seems that the all the chosen variables are quite well divided into clusters as you can see from the division of red and black colours. Especially variable zn, i.e., proportion of residential land zoned for lots over 25,000 sq.ft, is well divided into clusters where the other cluster, in black is mostly 0 in all the pairs.

## Bonus exercise code



```{r message=FALSE, warning=FALSE}
# reloading the dataset
library(MASS)
data("Boston")

# standardizing the dataset
scaled_boston <- scale(Boston)
scaled_data <- as.data.frame(scaled_boston)

# k-means with 3 clusters

km <- kmeans(scaled_data, centers = 3)

# new data with clusters assigned

data_with_clusters <- cbind(scaled_data, cluster = km$cluster)

# removing the chas variable as it is constant between the groups and interfering with the analysis

data_with_clusters <- dplyr::select(data_with_clusters, -chas)

# LDA with clusters as target classes

lda.bonus <- lda(cluster ~ ., data = data_with_clusters)
lda.bonus

# plotting the analysis

lda.arrows <- function(x, myscale = 1, arrow_heads = 0.2, color = "black", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)}

classes <- as.numeric(data_with_clusters$cluster)

plot(lda.fit, dimen = 2, col = classes)
lda.arrows(lda.bonus, myscale = 1)

```


Thats all folks.






